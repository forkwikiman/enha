General-Purpose computing on Graphics Processing Units

<del>본격 노가다와 사무직의 분리</del>

'[GPU](GPU.md)를 이용한 범용연산'의 머릿글자. [CPU](CPU.md)가 맡던 연산기능을 GPU에게 떠넘겨 연산속도를
향상시키는 기술을 말한다. GPU는 대량 계산에 용이하게 설계되기 때문에, 속도를 향상시킬 수 있다. 예를 들어,
[BOINC](BOINC.md) 중 몇몇 프로젝트는 CPU만으로는 10시간 이상이 걸리는 반면, GPGPU를 사용하면 2시간에서
3시간만에 프로젝트가 완료되는 경우를 볼 수 있다. Autodesk 3dsmax 라는 3D 프로그램에서도 CPU를 사용하면 5시간 이상
걸리는 인테리어 렌더링을 GPGPU로 연산을 시키면 약 20배 더 빨리 연산을 한다.(GPGPU 코어가 많을수록 속도는 배가 된다.)
레이트레이싱(라디오서티)등의 샘플 수를 늘려도 CPU 보다 빠르다. 자연스럽게
[슈퍼컴퓨터](%EC%8A%88%ED%8D%BC%EC%BB%B4%ED%93%A8%ED%84%B0.md)가 생각날 텐데, 실제로
2013년 현재 가장 빠른 중국의 텐허 슈퍼컴퓨터가 GPU 하이브리드 아키텍처를 사용하고 있다. 실은 여러분이 좀 비싼 그래픽카드를 샀다
싶으면 GPGPU없던 시절의 초 고성능 워크스테이션이나 조금 더 옛날의
[슈퍼컴퓨터](%EC%8A%88%ED%8D%BC%EC%BB%B4%ED%93%A8%ED%84%B0.md)와 동급의 괴물을 집안에 한 대
모셔둔 셈이다. CPU가 날고 기어도 **300**GFlops대인데(이게 무려 Core **i7 5960X**의 속도다.) 2015년 기준
**10만원대**의 GPU는 거의 대부분 1TFlops를 넘어가고 일부 GPU`[1]`는 이미 **10TFlops** 를 돌파했다!

그러면 GPU를 생산하는 [nVIDIA](nVIDIA.md)나 [AMD](AMD.md)가
[인텔](%EC%9D%B8%ED%85%94.md)이 [외계인고문](%EC%99%B8%EA%B3%84%EC%9D%B8%20%EA%B3%A0%EB%AC%B8.md)할 동안 아예 외계인 모선이라도
정ㅋ벅ㅋ 한거냐 하면 그건 아니고, 구조 자체가 [CPU](CPU.md)하고는 전혀 다르다. CPU는 다양한 환경에서의 작업을 빠르게
수행하기 위해 실제 연산을 수행하는 부분(ALU라고 한다)의 구조가 복잡하고 명령어 하나로 처리할 수 있는 기능도 많으며`[2]` 각종 제어
처리를 위한 부분이 매우 많은데 반해 GPU는 특화된 연산을 빠른 속도로 수행하기 위해 그런 부분을 과감히 삭제하고 비교적 단순한 다수의
ALU에 몰빵하는 구조로 만들어졌기 때문. 공학용 전자계산기 수백대를 때려넣었다고 보면 좋다. 때문에 GPU단독으로는 어떤 작업도 처리할 수
없다. GPU를 **제어**하는 건 여전히 CPU의 역할이다. 비유하자면 CPU는 천재 엔지니어 몇 명이 모인 엘리트집단이고 GPU는
막노동꾼이 모인 인력시장. 막노동꾼 백만 명이 있어도 그들을 통솔할 사람이 없으면 [부르즈할리파](%EB%B6%80%EB%A5%B4%EC%A6%88%20%ED%95%A0%EB%A6%AC%ED%8C%8C.md)는 못 짓는다.
거꾸로 천재 엔지니어에게 삽자루 쥐어주면 만들 수는 있다. 다음 세기 즈음에...

CPU와 GPU의 차이는  

![cpu_and_gpu_architecture_comparison.png](//rv.wkcdn.net/http://rigvedawiki.n
et/r1/pds/GPGPU/cpu_and_gpu_architecture_comparison.png)

[PNG image (28.16 KB)]

  
이런 차이가 있다. [CPU](CPU.md)는 커다란 코어를 4개 넣어놨지만 [GPU](GPU.md)는 작은놈으로 몇백개씩
때려박은 것을 볼 수 있다. 코어 하나의 처리속도는 CPU의 코어가 훨씬 빠르지만 GPU는 물량으로 커버한다. 따라서 GPGPU
프로그래밍시에는 CPU에서 프로그래밍할 때의 상식 하나를 정반대로 적용해야 하는데, CPU와는 다르게 **쓰레드를 가능한 한 많이
만들라**이다. [CPU](CPU.md)에서는 태스크 스위칭 때문에 코어수보다 많은(하이퍼쓰레딩시 코어수*2개) 쓰레드를 생성하면
성능이 떨어지지만 GPU에서는 쓰레드=다다익선이다.

GPU에게 연산을 시키려면 몇 가지 하드웨어 요구사항을 충족해야 하는데, 첫번째로는 프로그램 가능한 셰이더가 있어야 한다. 이렇게 프로그램
가능한 셰이더는 그래픽 카드가 기본 지원하지 않는 셰이더도 그릴 수 있어 더 많은 표현(렌즈 효과, 변위 매핑, 필드 깊이 등)을 할 수
있는데 이 부분을 이용하여 연산을 하게 된다. 또한, 데이터 자료형의 추가도 필요하다. 일반 그래픽 응용프로그램처럼 모든 계산이 행렬식으로
처리되지는 않기 때문이다. 이렇게 얘기하면 무슨 특별한 그래픽카드가 필요한 것처럼 들리겠지만 사실 여러분이 요즘 시중에서 구할 수 있는
사실상 모든 그래픽카드가 GPGPU를 지원한다. 내장 그래픽도 샌디브릿지부터 Directcompute를 지원한다.
[AMD](AMD.md)의 그래픽카드를 사면 CPU를 덤으로 주는 [APU](APU.md)들은 말할것도 없고.

GPU의 경우 이런 계산들을 빨리빨리 처리할 수 있도록 병렬로 처리하고 있으며, 또한 처리속도도 빠르게 올라가고 있다. 그러나, CPU와
GPU가 처리하는 방식은 여러모로 다르기에 이를 해결하기 위해 여러 방식들이 속속들이 개발되고 있다. GPGPU로 CPU 한 개에 비해 최대
100배~250배의 속도 향상을 이룰 수 있지만, 병렬도가 지극히 높은 응용 프로그램에서만 이 정도의 혜택을 볼 수 있다. 연산의 병렬도가
거의 없고 연산식과 데이터가 함께 바뀌는 응용에서는 오히려 CPU가 압도적으로 빠르다. 샌디브릿지 CPU가 터보부스트 모드일 때의 클럭은
4GHz에 육박할 정도가 되지만 GPU 코어 클럭은 이제야 1GHz에 도달한 정도에 불과하다. 게다가 CPU는 처음부터 연산식과 데이터가
지멋대로 튀는 환경을 가정해서 설계했지만 GPU에게 이런 응용을 던져주면 제어부가 CPU에 비해 안습이기 때문에 정작 일을 해야 할
ALU부분이 **놀고있다**. 대체적으로 GPU에서 돌아갈 코드에 if문을 하나 사용할 때마다 가용 자원을 절반씩 깎아먹는다고 봐도 된다.
if문은 코드의 흐름을 **두 개로 갈라놓는** 역할을 하기 때문에 이런 건 CPU쪽에서 가능한 한 처리해주는 게 좋다. 그렇지 않으면
if문의 반대편 절반에 해당하는 데이터는 처리되지 못하고 대기하다가 앞쪽 절반에 해당하는 데이터가 다 처리된 뒤에야 비로소 계산을 재개한다.
물론 if문의 한쪽에 데이터의 99%가 몰려있다거나 데이터양 자체가 충분히 많아서 절반쯤 나뉘어도 GPU의 자원을 전부 사용하는 경우에는
써도 성능에 큰 지장은 없다.

GPGPU를 돌리기 위해서는 저런 프로그램 가능한 GPU뿐만 아니라, 소프트웨어 적으로 변환시키는 레이어 혹은 라이브러리가 필요한데,
[CUDA](CUDA.md)와 [OpenCL](OpenCL.md)이 많이 알려져 있다. 그 외에 [마이크로소프트](%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EC%86%8C%ED%94%84%ED%8A%B8.md)에서 만든
DirectCompute가 있다.`[3]`

당연하지만, GPGPU를 가동할 때 그래픽카드에 부하가 많이 걸린다. 게임 돌리는 중에는 그 성능의 반도 안쓰는 경우가 많지만
GPGPU돌리고 있는 와중에는 그 계산 끝날때까지 100%를 유지하기 때문(물론 프로그램이 제대로 된 경우에만). 전기도 평소보다 훨씬 많이
쳐먹는다. 매뉴얼에 적혀있는 바로 그만큼의 전력을 진짜로 쓴다고 보면 된다. 그래서 [묻지마파워](%EB%AC%BB%EC%A7%80%EB%A7%88%20%ED%8C%8C%EC%9B%8C.md)같이 부실한 놈을 쓰면 이거
돌리다가 컴퓨터 통째로 말아먹는 수도 있다.

리얼타임 파티클, 리얼타임 레이트레이싱, 리얼타임 렌더링`[4]`, 유체 계산, 물리 엔진 등을 CPU보다 빨리 처리할 수 있어, 전문가용
프로그램만이 아닌, 게임에서도 이용할 수 있어 차세대 게임 프로세싱원으로 주목되고 있는 분야기도 하다. 위에서 언급했듯이 게임 중 오브젝트
수가 비교적 적은 경우에는 GPU가 절반 이상 놀고 있으므로 남는 자원을 파티클 처리에 더 투자한다던가 하는 식이다. 이미 피직스 등에서
GPU를 이용한 처리가 가능하며`[5]`, 콘솔에서도 [Wii U](Wii%20U.md)가 <del>부실한 CPU를 때우기
위해</del> GPGPU를 지원하고 있고 [플레이스테이션4](%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4%ED%85%8C%EC%9D%B4%EC%85%98%204.md)와
[엑스박스 원](%EC%97%91%EC%8A%A4%EB%B0%95%EC%8A%A4%20%EC%9B%90.md)도 비슷한 기능을 지원할
예정이다.`[6]`

AMD APU들의 궁극의 목표는 바로 저 GPGPU를 통해 **메인 연산장치 CPU + 보조 연산장치 내장 그래픽 칩**의 조합으로 기존의
CPU 아키텍쳐에서는 상상도 못할 CPU 성능을 뽑아내는 것이다. 그 첫단계로 적용된 것이 기존의 GPGPU 난이도를 내장 그래픽 칩
한정으로 대폭 줄인다는 목적으로 카베리부터 들어가는 이기종 시스템 아키텍쳐-**[HSA](HSA.md)**. 실제로 첫 HSA 제품인
카베리 A10-7850K의 경우 HSA 환경 구성시 i3와 경쟁하던 성능에서 i5-4670를 제껴버리는 경이로운 성능 부스트를 보여주며 그
가능성을 증명했다.`[7]`

`\----`

  * `[1]` R9 295X2
  * `[2]` CPU에 계속 추가되고 있는 확장 명령어(SSE 등)을 보면 명령어 하나로 계산 여러개를 한꺼번에 하거나 복잡한 수식 처리를 하기 위한 것이 많다. 예를 들어 곱하기 8개나 벡터곱 연산을 한번에 한다거나...
  * `[3]` [DXVA](DXVA.md)가 DirectCompute를 이용한 동영상 가속 기능이다.
  * `[4]` 제대로된 GPU 기반의 프로그램으로 렌더링을 할경우 CPU보다 몇배~몇십배 빠른 렌더링이 가능하다. V-ray RT나 Octane Render 참고.
  * `[5]` 아예 SLI 환경에서 그래픽 카드 하나를 피직스 용으로 이용할 수도 있다. 특히 비공식적으로 라데온 그래픽카드에 지포스 그래픽카드를 덤으로 끼워서 피직스를 구현하는 하이브리드 피직스의 경우 화면출력은 라데온이, 피직스 계산은 지포스 쪽에서 처리시키는 구조다.
  * `[6]` [플레이스테이션 3](%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4%ED%85%8C%EC%9D%B4%EC%85%98%203.md)도 [CELL](CELL-Broadband%20Engine.md)의 SPU를 이용하는 방식이 GPGPU와 유사한 면이 있다. 애초에 플레이스테이션 3은 개발 당시 GPU를 따로 안 쓰고 CELL에서 다 처리하려 했기 때문에 구조가 유사하다. 결국 실패하여 nVidia의 RSX가 달렸지만.
  * `[7]` 그 덕분에 카베리 APU들의 경우 코어수를 표시할때 기존의 CPU 코어만 표기하는 대신 아예 그래픽 코어까지 묶어서 컴퓨트 코어라는 단어를 쓰게되었다. A10-7850K의 경우 CPU 4+GPU 8로 12 컴퓨트 코어로 표기. 다만 카베리가 나온 2014년에는 HSA 지원 소프트웨어는 AMD 자체적으로 제공하는 한두개를 제외하면 없고 벤치마크 테스트 등에서나 바이오스 세팅부터 시작되는 온갖 삽질을 다 하면서 HSA 환경으로 세팅해서 돌려보는 정도밖에 되지 않는다.

